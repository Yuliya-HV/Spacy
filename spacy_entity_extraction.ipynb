{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy for Entity Extraction tasks. From token to statistical model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install spacy type in terminal in desired environment:\n",
    "    \n",
    "    pip install spacy\n",
    "    \n",
    "To install specific language model:\n",
    "\n",
    "    python -m spacy download en_core_web_lg \n",
    "    \n",
    "where en_core_web_lg is a model name. There are few types available: _sm - small, _md - medium, _lg - large.\n",
    "\n",
    "Note! Small models don't contain word vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "\n",
    "from spacy.lang.en import English\n",
    "#from spacy.lang.el import Greek\n",
    "\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic document processing  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial process document using spaCy default pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 I\n",
      "1 use\n",
      "2 spaCy\n",
      "3 version\n",
      "4 2.2.4\n",
      "5 !\n"
     ]
    }
   ],
   "source": [
    "#Create basic spacy object for English language. Similar for any supported language.\n",
    "nlp = English()\n",
    "\n",
    "#Another way: to load small English/ model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#run default spacy pipeline to process a document. Document transforms into object spacy.tokens.doc.Doc.\n",
    "doc = nlp('I use spaCy version 2.2.4!')\n",
    "\n",
    "#iterate through tokens in the document\n",
    "for token in doc:\n",
    "    print(token.i, token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spaCy'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract specific token by index\n",
    "token = doc[2]\n",
    "token.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'version 2.2.4'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get a phrase slicing the document\n",
    "span = doc[3:5]\n",
    "span.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Token object: getting part of speech, dependencies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look how to get token's text, part of speech, dependency label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I use spaCy version 2.2.4! \n",
      "Selected token: use \n",
      "Part of speech: VERB \n",
      "Dependency label: ROOT\n"
     ]
    }
   ],
   "source": [
    "print('Sentence: {} \\nSelected token: {} \\nPart of speech: {} \\nDependency label: {}'\\\n",
    "      .format(doc, doc[1].text, doc[1].pos_, doc[1].dep_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document representation: ['I', 'use', 'spaCy', 'version', '2.2.4', '!']\n",
      "Token indexes: [0, 1, 2, 3, 4, 5]\n",
      "Token is alpha: [True, True, True, True, False, False]\n",
      "Token is punctuation: [False, False, False, False, False, True]\n",
      "Token is a number: [False, False, False, False, True, False]\n"
     ]
    }
   ],
   "source": [
    "print('Document representation: {}\\nToken indexes: {}\\nToken is alpha: {}\\nToken is punctuation: {}\\n\\\n",
    "Token is a number: {}'.format([token.text for token in doc],\n",
    "                              [token.i for token in doc], \n",
    "                              [token.is_alpha for token in doc],\n",
    "                              [token.is_punct for token in doc],\n",
    "                              [token.like_num for token in doc]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency visualisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"a95ab413cc664d53b0a91e1d1d01e412-0\" class=\"displacy\" width=\"1800\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">This</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">sentence</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">demonstrate</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">dependency</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">visualisation</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">spaCy.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a95ab413cc664d53b0a91e1d1d01e412-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a95ab413cc664d53b0a91e1d1d01e412-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a95ab413cc664d53b0a91e1d1d01e412-0-1\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a95ab413cc664d53b0a91e1d1d01e412-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a95ab413cc664d53b0a91e1d1d01e412-0-2\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a95ab413cc664d53b0a91e1d1d01e412-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,179.0 L583.0,167.0 567.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a95ab413cc664d53b0a91e1d1d01e412-0-3\" stroke-width=\"2px\" d=\"M770,177.0 C770,89.5 920.0,89.5 920.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a95ab413cc664d53b0a91e1d1d01e412-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,179.0 L762,167.0 778,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a95ab413cc664d53b0a91e1d1d01e412-0-4\" stroke-width=\"2px\" d=\"M595,177.0 C595,2.0 925.0,2.0 925.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a95ab413cc664d53b0a91e1d1d01e412-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">relcl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M925.0,179.0 L933.0,167.0 917.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a95ab413cc664d53b0a91e1d1d01e412-0-5\" stroke-width=\"2px\" d=\"M1120,177.0 C1120,89.5 1270.0,89.5 1270.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a95ab413cc664d53b0a91e1d1d01e412-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,179.0 L1112,167.0 1128,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a95ab413cc664d53b0a91e1d1d01e412-0-6\" stroke-width=\"2px\" d=\"M945,177.0 C945,2.0 1275.0,2.0 1275.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a95ab413cc664d53b0a91e1d1d01e412-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1275.0,179.0 L1283.0,167.0 1267.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a95ab413cc664d53b0a91e1d1d01e412-0-7\" stroke-width=\"2px\" d=\"M1295,177.0 C1295,89.5 1445.0,89.5 1445.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a95ab413cc664d53b0a91e1d1d01e412-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1445.0,179.0 L1453.0,167.0 1437.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a95ab413cc664d53b0a91e1d1d01e412-0-8\" stroke-width=\"2px\" d=\"M1470,177.0 C1470,89.5 1620.0,89.5 1620.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a95ab413cc664d53b0a91e1d1d01e412-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1620.0,179.0 L1628.0,167.0 1612.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(\"This is a sentence to demonstrate dependency visualisation in spaCy.\")\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training example 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token          POS       DEP       \n",
      "\n",
      "This           DET       nsubj     \n",
      "is             AUX       ROOT      \n",
      "a              DET       det       \n",
      "sentence       NOUN      attr      \n",
      "to             PART      aux       \n",
      "demonstrate    VERB      relcl     \n",
      "dependency     NOUN      compound  \n",
      "visualisation  NOUN      dobj      \n",
      "in             ADP       prep      \n",
      "spaCy          NOUN      pobj      \n",
      ".              PUNCT     punct     \n"
     ]
    }
   ],
   "source": [
    "print('{:<15}{:<10}{:<10}\\n'.format(\"Token\", \"POS\", 'DEP'))\n",
    "\n",
    "for token in doc:\n",
    "    # Get the token text, part-of-speech tag and dependency label\n",
    "    token_text = token.text\n",
    "    token_pos = token.pos_\n",
    "    token_dep = token.dep_\n",
    "    print('{:<15}{:<10}{:<10}'.format(token_text, token_pos, token_dep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we need to get all percentages from a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage found: 38, 42, 40.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"There is a chance of rain 38% before 1 p.m. Cloudy, with a high near 42%.\\\n",
    "            West northwest wind 7 to 13 mph, with gusts as high as 23 mph. Chance of precipitation is 40%.\")\n",
    "tokens = list()\n",
    "for token in doc:\n",
    "    if token.like_num:\n",
    "        if doc[token.i + 1].text == '%':\n",
    "            tokens.append(token.text)\n",
    "print('Percentage found: {}.'.format(', '.join(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Named Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US GPE\n",
      "Apple Inc. ORG\n",
      "AAPL ORG\n",
      "first ORDINAL\n",
      "$1 trillion MONEY\n",
      "$1.3 trillion MONEY\n",
      "Dec. 2019 DATE\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"US company Apple Inc. (AAPL) became the world's first company to record a market capitalization of $1 trillion, and subsequently passed the $1.3 trillion threshold in Dec. 2019\")\n",
    "\n",
    "#access to predicted NE through .ents\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link entities with tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BBC News\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is a \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    British\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " free-to-air television news channel. It was launched as \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BBC News\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    24\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " on \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    9 November 1997\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " at \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    5:30 pm\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TIME</span>\n",
       "</mark>\n",
       " as part of the \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BBC\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "'s foray into digital domestic television channels, becoming the \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    first\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
       "</mark>\n",
       " competitor to \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Sky News\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", which had been running since \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1989\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"BBC News is a British free-to-air television news channel. It was launched as BBC News 24 on 9 November 1997 at 5:30 pm as part of the BBC's foray into digital domestic television channels, becoming the first competitor to Sky News, which had been running since 1989.\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some help hints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a clear understanding what a spacy abbreviation means you can use the following method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pronoun',\n",
       " '\"first\", \"second\", etc.',\n",
       " 'Numerals that do not fall under another type')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('PRON'), spacy.explain('ORDINAL'), spacy.explain('CARDINAL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule-based matching (more powerful than RegEx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a pattern to capture some patterns in a text we can do following:\n",
    "    1. Create Matcher object\n",
    "    2. Defind a pattern\n",
    "    3. Add pattern to the Matcher\n",
    "    4. Apply the Matcher for a document\n",
    "    5. Get result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialise Matcher object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create own pattern\n",
    "\n",
    "For example you want to match 'MacBook Pro' in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found matches: ['MacBook Pro', 'MacBook Pro']\n"
     ]
    }
   ],
   "source": [
    "#This pattern will match MacBook and Pro next to it\n",
    "pattern = [{'TEXT': 'MacBook'}, {'TEXT': 'Pro'}]\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "matcher.add('MacBook_PATTERN', None, pattern)\n",
    "\n",
    "# Use the matcher on the document\n",
    "doc = nlp('Designed for those who defy limits and change the world, the new MacBook Pro is by far the most powerful notebook we’ve ever made. With an immersive 16-inch Retina display, super-fast processors, next-generation graphics, the largest battery capacity ever in a MacBook Pro, a new Magic Keyboard and massive storage, it’s the ultimate pro notebook for the ultimate user.')\n",
    "matches = matcher(doc)\n",
    "\n",
    "print('Found matches:', [doc[start:end].text for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example. Next pattern will spot a number and a noun, so we can find number of bedrooms/ bathrooms in the apartment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NUM'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp('2 bedrooms')[0].pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'DET'], ['apartment', 'NOUN'], ['features', 'VERB'], ['2', 'NUM'], ['bedrooms', 'NOUN'], [',', 'PUNCT'], ['a', 'DET'], ['fully', 'ADV'], ['equipped', 'VERB'], ['kitchen', 'NOUN'], ['with', 'ADP'], ['a', 'DET'], ['dining', 'NOUN'], ['area', 'NOUN'], ['and', 'CCONJ'], ['dishwasher', 'NOUN'], [',', 'PUNCT'], ['2', 'NUM'], ['bathrooms', 'NOUN'], [',', 'PUNCT'], ['and', 'CCONJ'], ['a', 'DET'], ['living', 'NOUN'], ['room', 'NOUN'], ['with', 'ADP'], ['a', 'DET'], ['flat', 'ADJ'], ['-', 'PUNCT'], ['screen', 'NOUN'], ['TV', 'NOUN'], ['.', 'PUNCT']]\n",
      "Match found: 2\n",
      "Match found: 2 bedrooms\n",
      "Match found: 2\n",
      "Match found: 2 bathrooms\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"The apartment features 2 bedrooms, a fully equipped kitchen with a dining area and dishwasher, 2 bathrooms, and a living room with a flat-screen TV.\")\n",
    "\n",
    "print([[t.text, t.pos_] for t in doc])\n",
    "# Write a pattern for full iOS versions (\"iOS 7\", \"iOS 11\", \"iOS 10\")\n",
    "pattern = [{'IS_DIGIT': True}, {'POS': 'NOUN'}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add('IOS_VERSION_PATTERN', None, pattern)\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print('Match found:', doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 3\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I downloaded Fortnite on my laptop and can't open the game at all. \\\n",
    "           Help? so when I was downloading Minecraft, I got the Windows version \\\n",
    "           where it is the '.zip' folder and I used the default program to unpack \\\n",
    "           it... do I also need to download Winzip?\")\n",
    "\n",
    "# Write a pattern that matches a form of \"download\" plus proper noun\n",
    "pattern = [{'LEMMA': 'download'}, {'POS': 'PROPN'}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add('DOWNLOAD_PATTERN', None, pattern)\n",
    "matches = matcher(doc)\n",
    "print('Total matches found:', len(matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 5\n",
      "Match found: beautiful design\n",
      "Match found: smart search\n",
      "Match found: automatic labels\n",
      "Match found: optional voice\n",
      "Match found: optional voice responses\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Features of the app include a beautiful design, smart search, automatic labels and optional voice responses.\")\n",
    "\n",
    "# Write a pattern for adjective plus one or two nouns\n",
    "pattern = [{'POS': 'ADJ'}, {'POS': 'NOUN'}, {'POS': 'NOUN', 'OP': '?'}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add('ADJ_NOUN_PATTERN', None, pattern)\n",
    "matches = matcher(doc)\n",
    "print('Total matches found:', len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print('Match found:', doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3197928453018144401, 'coffee')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('I like coffee')\n",
    "coffee_hash = nlp.vocab.strings['coffee']\n",
    "\n",
    "#hash cannot be stored, so error\n",
    "#coffee_string = nlp.vocab.strings[coffee_hash]\n",
    "\n",
    "coffee_hash, nlp.vocab.strings[3197928453018144401]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee 3197928453018144401 True\n"
     ]
    }
   ],
   "source": [
    "lexeme = nlp.vocab['coffee']\n",
    "\n",
    "print(lexeme.text, lexeme.orth, lexeme.is_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Span object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "words = ['Hello', 'world', '!']\n",
    "spaces = [True, False, False]\n",
    "\n",
    "#create a document manually\n",
    "doc = Doc(nlp.vocab, words = words, spaces = spaces)\n",
    "\n",
    "#create a span manually\n",
    "span = Span(doc, 0, 2)\n",
    "\n",
    "span_with_label = Span(doc, 0, 2, label = 'GREETING')\n",
    "\n",
    "#add span to doc.ents, it is overwritable\n",
    "doc.ents = [span_with_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go, get started!\n"
     ]
    }
   ],
   "source": [
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Desired text: \"Go, get started!\"\n",
    "words = ['Go', ',', 'get', 'started', '!']\n",
    "spaces = [False, True, True, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "David Bowie PERSON\n"
     ]
    }
   ],
   "source": [
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# Create a doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=['I', 'like', 'David', 'Bowie'], spaces=[True, True, True, False])\n",
    "\n",
    "# Create a span for \"David Bowie\" from the doc and assign it the label \"PERSON\"\n",
    "span = Span(doc, 2, 4, label=\"PERSON\")\n",
    "print(span.text, span.label_)\n",
    "\n",
    "# Add the span to the doc's entities\n",
    "doc.ents = [span]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all tokens and part-of-speech tags\n",
    "pos_tags = [token.pos_ for token in doc]\n",
    "\n",
    "for index, pos in enumerate(pos_tags):\n",
    "    # Check if the current token is a proper noun\n",
    "    if pos == 'PROPN':\n",
    "        # Check if the next token is a verb\n",
    "        if pos_tags[index + 1] == 'VERB':\n",
    "            print('Found a verb after a proper noun!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all tokens and part-of-speech tags\n",
    "\n",
    "for token in doc:\n",
    "    # Check if the current token is a proper noun\n",
    "    if token.pos_ == 'PROPN':\n",
    "        # Check if the next token is a verb\n",
    "        if doc[token.i + 1].pos_ == 'VERB':\n",
    "            print('Found a verb after a proper noun!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8510907303318979\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp('I don\\'t like fast food')\n",
    "doc2 = nlp('I like pizza')\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7369546\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I like pizza and pasta\")\n",
    "token1 = doc[2]\n",
    "token2 = doc[4]\n",
    "print(token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.2009e-01 -3.0322e-02 -7.9859e-02 -4.6279e-01 -3.8600e-01  3.6962e-01\n",
      " -7.7178e-01 -1.1529e-01  3.3601e-02  5.6573e-01 -2.4001e-01  4.1833e-01\n",
      "  1.5049e-01  3.5621e-01 -2.1508e-01 -4.2743e-01  8.1400e-02  3.3916e-01\n",
      "  2.1637e-01  1.4792e-01  4.5811e-01  2.0966e-01 -3.5706e-01  2.3800e-01\n",
      "  2.7971e-02 -8.4538e-01  4.1917e-01 -3.9181e-01  4.0434e-04 -1.0662e+00\n",
      "  1.4591e-01  1.4643e-03  5.1277e-01  2.6072e-01  8.3785e-02  3.0340e-01\n",
      "  1.8579e-01  5.9999e-02 -4.0270e-01  5.0888e-01 -1.1358e-01 -2.8854e-01\n",
      " -2.7068e-01  1.1017e-02 -2.2217e-01  6.9076e-01  3.6459e-02  3.0394e-01\n",
      "  5.6989e-02  2.2733e-01 -9.9473e-02  1.5165e-01  1.3540e-01 -2.4965e-01\n",
      "  9.8078e-01 -8.0492e-01  1.9326e-01  3.1128e-01  5.5390e-02 -4.2423e-01\n",
      " -1.4082e-02  1.2708e-01  1.8868e-01  5.9777e-02 -2.2215e-01 -8.3950e-01\n",
      "  9.1987e-02  1.0180e-01 -3.1299e-01  5.5083e-01 -3.0717e-01  4.4201e-01\n",
      "  1.2666e-01  3.7643e-01  3.2333e-01  9.5673e-02  2.5083e-01 -6.4049e-02\n",
      "  4.2143e-01 -1.9375e-01  3.8026e-01  7.0883e-03 -2.0371e-01  1.5402e-01\n",
      " -3.7877e-03 -2.9396e-01  9.6518e-01  2.0068e-01 -5.6572e-01 -2.2581e-01\n",
      "  3.2251e-01 -3.4634e-01  2.7064e-01 -2.0687e-01 -4.7229e-01  3.1704e-01\n",
      " -3.4665e-01 -2.5188e-01 -1.1201e-01 -3.3937e-01  3.1518e-01 -3.2221e-01\n",
      " -2.4530e-01 -7.1571e-02 -4.3971e-01 -1.2070e+00  3.3365e-01 -5.8208e-02\n",
      "  8.0899e-01  4.2335e-01  3.8678e-01 -6.0797e-01 -7.3760e-01 -2.0547e-01\n",
      " -1.7499e-01 -3.7842e-03  2.1930e-01 -5.2486e-02  3.4869e-01  4.3852e-01\n",
      " -3.4471e-01  2.8910e-01  7.2554e-02 -4.8625e-01 -3.8390e-01 -4.4760e-01\n",
      "  4.3278e-01 -2.7128e-03 -9.0067e-01 -3.0819e-02 -3.8630e-01 -8.0798e-02\n",
      " -1.6243e-01  2.8830e-01 -2.6349e-01  1.7628e-01  3.5958e-01  5.7672e-01\n",
      " -5.4624e-01  3.8555e-02 -2.0182e+00  3.2916e-01  3.4672e-01  1.5398e-01\n",
      " -4.3446e-01 -4.1428e-02 -6.9588e-02  5.1513e-01 -1.3489e-01 -5.7239e-02\n",
      "  4.9241e-01  1.8643e-01  3.8596e-01 -3.7329e-02 -5.4216e-01 -1.8152e-01\n",
      "  4.3110e-01 -4.6967e-01  6.6801e-02  5.0323e-01 -2.4059e-01  3.6742e-01\n",
      "  2.9300e-01 -8.7883e-02 -4.7940e-01 -4.3431e-02 -2.6137e-01 -6.2658e-01\n",
      "  1.1446e-01  2.7682e-01  3.4800e-01  5.0018e-01  1.4269e-01 -3.3545e-01\n",
      " -3.9712e-01 -3.3121e-01 -3.4434e-01 -4.1627e-01 -3.5707e-03 -6.2350e-01\n",
      "  3.7794e-01 -1.6765e-01 -4.1954e-01 -3.3134e-01  3.1232e-01 -3.9494e-01\n",
      " -4.6921e-03 -4.8884e-01 -2.2059e-02 -2.6174e-01  1.7937e-01  3.6628e-01\n",
      "  5.8971e-02 -3.5991e-01 -4.4393e-01 -1.1890e-01  3.3487e-01  3.6505e-02\n",
      " -3.2788e-01  3.3425e-01 -5.6361e-01 -1.1190e-01  5.3770e-01  2.0311e-01\n",
      "  1.5110e-01  1.0623e-02  3.3401e-01  4.6084e-01  5.6293e-01 -7.5432e-02\n",
      "  5.4813e-01  1.9395e-01 -2.6265e-01 -3.1699e-01 -8.1778e-01  5.8169e-02\n",
      " -5.7866e-02 -1.1781e-01 -5.8742e-02 -1.4092e-01 -9.9394e-01 -9.4532e-02\n",
      "  2.3503e-01 -4.9027e-01  8.5832e-01  1.1540e-01 -1.5049e-01  1.9065e-01\n",
      " -2.6705e-01  2.5326e-01 -6.7579e-01 -1.0633e-02 -5.5158e-02 -3.1004e-01\n",
      " -5.8036e-02 -1.7200e-01  1.3298e-01 -3.2899e-01 -7.5481e-02  2.9425e-02\n",
      " -3.2949e-01 -1.8691e-01 -9.5323e-01 -3.5468e-01 -3.3162e-01  5.6441e-02\n",
      "  2.1790e-02  1.7182e-01 -4.4267e-01  6.9765e-01 -2.6876e-01  1.1659e-01\n",
      " -1.6584e-01  3.8296e-01  2.9109e-01  3.6318e-01  3.6961e-01  1.6305e-01\n",
      "  1.8152e-01  2.2453e-01  3.9866e-02 -3.7607e-02 -3.6089e-01  7.0818e-02\n",
      " -2.1509e-01  3.6551e-01 -5.1603e-01 -5.8102e-03 -4.8320e-01 -2.5068e-01\n",
      " -5.2062e-02 -2.0828e-01  2.9060e-01  2.2084e-02 -6.8123e-01  4.2063e-01\n",
      "  9.5973e-02  8.1720e-01 -1.5241e-01  6.2994e-01  2.6449e-01 -1.3516e-01\n",
      "  3.2450e-01  3.0503e-01  1.2357e-01  1.5107e-01  2.8327e-01 -3.3838e-01\n",
      "  4.6106e-02 -1.2361e-01  1.4516e-01 -2.7947e-02  2.6231e-02 -5.9591e-01\n",
      " -4.4183e-01  7.8440e-01 -3.4375e-02 -1.3928e+00  3.5248e-01  6.5220e-01]\n"
     ]
    }
   ],
   "source": [
    "# Process a text\n",
    "doc = nlp(\"Two bananas in pyjamas\")\n",
    "\n",
    "# Get the vector for the token \"bananas\"\n",
    "bananas_vector = doc[1].vector\n",
    "print(bananas_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8789265574516525\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"It's a warm summer day\")\n",
    "doc2 = nlp(\"It's sunny outside\")\n",
    "\n",
    "# Get the similarity of doc1 and doc2\n",
    "similarity = doc1.similarity(doc2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22325331\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"TV and books\")\n",
    "token1, token2 = doc[0], doc[2]\n",
    "\n",
    "# Get the similarity of the tokens \"TV\" and \"books\" \n",
    "similarity = token1.similarity(token2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75173926\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"This was a great restaurant. Afterwards, we went to a really nice bar.\")\n",
    "\n",
    "# Create spans for \"great restaurant\" and \"really nice bar\"\n",
    "span1 = doc[3:5]\n",
    "span2 = doc[12:15]\n",
    "\n",
    "# Get the similarity of the spans\n",
    "similarity = span1.similarity(span2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span:  Golden Retriever\n",
      "Root token: Retriever\n",
      "Root head token: have\n",
      "Previous token: a DET\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('DOG', None, [{'LOWER': 'golden'}, {'LOWER': 'retriever'}])\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print(\"Matched span: \", span.text)\n",
    "    print('Root token:', span.root.text)\n",
    "    print('Root head token:', span.root.head.text)\n",
    "    print('Previous token:', doc[start-1].text, doc[start-1].pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the match patterns\n",
    "pattern1 = [{'LOWER': 'amazon'}, {'IS_TITLE': True, 'POS': 'PROPN'}]\n",
    "pattern2 = [{'LOWER': 'ad'}, {'TEXT': '-'}, {'LOWER': 'free'}, {'POS': 'NOUN'}]\n",
    "\n",
    "# Initialize the Matcher and add the patterns\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('PATTERN1', None, pattern1)\n",
    "matcher.add('PATTERN2', None, pattern2)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Print pattern string name and text of matched span\n",
    "    print(doc.vocab.strings[match_id], doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the PhraseMatcher and initialize it\n",
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "doc = nlp('Czech Republic may help Slovakia protect its airspace')\n",
    "\n",
    "# Create pattern Doc objects and add them to the matcher\n",
    "# This is the faster version of: [nlp(country) for country in COUNTRIES]\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add('COUNTRY', None, *patterns)\n",
    "\n",
    "# Call the matcher on the test document and print the result\n",
    "matches = matcher(doc)\n",
    "print([doc[start:end] for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tagger', <spacy.pipeline.pipes.Tagger object at 0x121cf9100>), ('parser', <spacy.pipeline.pipes.DependencyParser object at 0x12bbd8d60>), ('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x121d022e0>)]\n"
     ]
    }
   ],
   "source": [
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['length_component', 'tagger', 'parser', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# Define the custom component\n",
    "def length_component(doc):\n",
    "    # Get the doc's length\n",
    "    doc_length = len(doc)\n",
    "    print(\"This document is {} tokens long.\".format(doc_length))\n",
    "    # Return the doc\n",
    "    return doc\n",
    "\n",
    "# Load the small English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "  \n",
    "# Add the component first in the pipeline and print the pipe names\n",
    "nlp.add_pipe(length_component, first = True)\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is 5 tokens long.\n"
     ]
    }
   ],
   "source": [
    "# Define the custom component\n",
    "def length_component(doc):\n",
    "    # Get the doc's length\n",
    "    doc_length = len(doc)\n",
    "    print(\"This document is {} tokens long.\".format(doc_length))\n",
    "    # Return the doc\n",
    "    return doc\n",
    "  \n",
    "# Load the small English model and Add the component first in the pipeline\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe(length_component, first=True)\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"This is a sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a blank 'en' model\n",
    "nlp = spacy.blank('en')\n",
    "\n",
    "# Create a new entity recognizer and add it to the pipeline\n",
    "ner = nlp.create_pipe('ner')\n",
    "nlp.add_pipe(ner)\n",
    "\n",
    "# Add the label 'GADGET' to the entity recognizer\n",
    "ner.add_label('GADGET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the training\n",
    "nlp.begin_training()\n",
    "\n",
    "# Loop for 10 iterations\n",
    "for itn in range(10):\n",
    "    # Shuffle the training data\n",
    "    random.shuffle(TRAINING_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the training\n",
    "nlp.begin_training()\n",
    "\n",
    "# Loop for 10 iterations\n",
    "for itn in range(10):\n",
    "    # Shuffle the training data\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    losses = {}\n",
    "    \n",
    "    # Batch the examples and iterate over them\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA, size=2):\n",
    "        texts = [text for text, entities in batch]\n",
    "        annotations = [entities for text, entities in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the training\n",
    "nlp.begin_training()\n",
    "\n",
    "# Loop for 10 iterations\n",
    "for itn in range(10):\n",
    "    # Shuffle the training data\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    losses = {}\n",
    "    \n",
    "    # Batch the examples and iterate over them\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA, size=2):\n",
    "        texts = [text for text, entities in batch]\n",
    "        annotations = [entities for text, entities in batch]\n",
    "        \n",
    "        # Update the model\n",
    "        nlp.update(texts, annotations, losses=losses)\n",
    "        print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each text in TEST_DATA\n",
    "for doc in nlp.pipe(TEST_DATA):\n",
    "    # Print the document text and entitites\n",
    "    print(doc.text)\n",
    "    print(doc.ents, '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-interum-venv",
   "language": "python",
   "name": "nlp-interum-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
